{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AS4501\n",
    "\n",
    "# Bayesian inference\n",
    "\n",
    "Francisco Förster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes rule\n",
    "\n",
    "## $\\begin{align}\n",
    "P(D , \\theta| M) &= P(D | \\theta, M) P(\\theta | M) \\\\\n",
    "&= P(\\theta | D, M) P(D | M)\n",
    "\\end{align} $\n",
    "\n",
    "This implies\n",
    "\n",
    "## $P(\\theta | D, M) = \\frac{P(D | \\theta, M) P(\\theta | M)}{P(D | M)}$\n",
    "\n",
    "The terms are called as follows:\n",
    "\n",
    "\n",
    "* The **posterior**: $P(\\theta | D, M)$\n",
    "\n",
    "* The **likelihood**: $P(D | \\theta, M)$\n",
    "\n",
    "* The **prior**: $P(\\theta | M)$\n",
    "\n",
    "* The **evidence**: $P(D | M)$\n",
    "\n",
    "The evidence is usually assumed to be some normalization constant, independent of $\\theta$, but it has some applications.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This means that we have a way of computing the probability of the model parameters given some model, the data and some prior information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The posterior summarizes everything we know about the parameters given the data, including possible correlations between variables \n",
    "\n",
    "It is usually represented by a **corner plot** of the parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model example\n",
    "\n",
    "![](spec.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corner plot \n",
    "\n",
    "![](corner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example \n",
    "![](corner2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T10:41:45.209984Z",
     "start_time": "2018-12-04T10:41:45.205901Z"
    }
   },
   "source": [
    "The **Maximum a Posteriori (MAP)** corresponds to the solution which maximizes the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](MAP.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reporting posterior is the best we can do\n",
    "\n",
    "* The problems start when we have too many dimensions, remember the curse of dimensionality\n",
    "\n",
    "* Then, sampling the model parameters can become very expensive computationally\n",
    "\n",
    "* To solve this problem, different **sampling** algorithms have been developed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the cumulative distribution function (cdf)\n",
    "\n",
    "![](cdf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use rejection sampling\n",
    "\n",
    "![](rejectionsampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic idea:\n",
    "\n",
    "\n",
    "1. Create a **proposal distribution $q(x)$** which satisfies $M q(x) \\ge \\tilde{p}(x)$, where M is some constant and $\\tilde{p}(x)$ is the **target distribution**, some unnormalized version of $p(x)$. $M q(x)$ provides an upper envelope for $\\tilde{p}$\n",
    "\n",
    "2. Sample $x \\sim q(x)$, which corresponds to picking a random $x$ location under the envelope, and then we sample $u \\sim U(0, 1)$, whih corresponds to picking a random heigtht.\n",
    "\n",
    "3. If $u > \\frac{\\tilde{p}(x)}{M q(x)}$, reject the sample, otherwise accept it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define:\n",
    "\n",
    "## $ S = \\lbrace (x, u): u \\le \\tilde{p}(x)/M q(x) \\rbrace$\n",
    "\n",
    "and \n",
    "\n",
    "## $ S_0 = \\lbrace (x, u): x \\le x_0, u \\le \\tilde{p}(x) / M q(x) \\rbrace$\n",
    "\n",
    "\n",
    "\n",
    "The cdf of the accepted points is given by:\n",
    "\n",
    "## $\\begin{align}\n",
    "P(x \\le x_0 | x~\\rm{accepted}) &= \\frac{P(x \\le x_0, x~\\rm{accepted})}{P(x~\\rm{accepted})} \\\\\n",
    " &= \\frac{\\int \\int \\mathbb{I}((x, u) \\in S_0) q(x) du dx}{\\int \\int  \\mathbb{I}((x, u) \\in S) q(x) du dx} \n",
    " = \\frac{\\int\\limits_{-\\inf}^{x_0} \\tilde{p}(x)}{\\int\\limits_{-\\inf}^{\\inf} \\tilde{p}(x)}\n",
    "\\end{align}$\n",
    "\n",
    "Which is the cdf of $p(x)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application to Bayesian statistics:\n",
    "\n",
    "Suppose we want to draw unweighted samples from the posterior;\n",
    "\n",
    "### $p(\\theta | D) = p(D | \\theta) p(\\theta) / P(D)$.\n",
    "\n",
    "We can use rejection sampling with the following target distribution:\n",
    "\n",
    "### $\\tilde{p}(\\theta) = p(D | \\theta) p(\\theta)$,\n",
    "\n",
    "the following proposal distribution:\n",
    "\n",
    "### $q(\\theta) = p(\\theta)$\n",
    "\n",
    "and \n",
    "\n",
    "### $M = p(D | \\hat{\\theta})$,\n",
    "\n",
    "where $\\hat{\\theta} = arg \\max(p(D | \\theta))$,\n",
    "\n",
    "the maximum likelihood estimator of $\\theta$.\n",
    "\n",
    "Then, we accept points with probability\n",
    "\n",
    "### $\\frac{\\tilde{p}(\\theta)}{M q(\\theta)} = \\frac{p(D|\\theta)}{p(D | \\hat{\\theta})}$\n",
    "\n",
    "Thus, samples from the prior which have high likelihood are more likely to be retained in the posterior. \n",
    "\n",
    "However, if there is a big mismatch between prior and posterior this procedure is very inefficient. This is usually the case in high dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo (MCMC) sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Family of methods to sample distributions in high dimensions\n",
    "\n",
    "* Among top 10 most important algorithms of the century!\n",
    "\n",
    "* **Basic idea**:\n",
    "\n",
    "    * Construct a Markov chain (sequence of values where the next value $i+1$ depends only on the present value $i$) on the state space whose stationary distribution is the target density $p^*(x)$\n",
    "    \n",
    "    * That is, we perform a random walk on the state space in such a way that the fraction of time we spend in each state $x$ is proportional to $p^*(x)$.\n",
    "    \n",
    "    * By drawing correlated samples $x_0, x_1, x_2$ from the chain, we can perform Monte Carlo integration w.r.t. $p^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T12:26:40.541640Z",
     "start_time": "2018-12-04T12:26:40.530983Z"
    }
   },
   "source": [
    "* One of the most popular MCMC algorithm\n",
    "\n",
    "* **Basic  idea**:\n",
    "    \n",
    "    * we sample each variable in turn, conditioned on the values of all the other variables in the distribution\n",
    "        \n",
    "    * given a joint sample $x^s$ of all the variables, we generate a new sample $x^{s+1}$ by sampling each component in turn, based on the most recent values of the variables, e.g. for 3 dimensions:\n",
    "    \n",
    "        * $x_1^{s+1} \\sim p(x_1 | x_2^s, x_3^s)$\n",
    "        * $x_2^{s+1} \\sim p(x_2 | x_1^s, x_3^s)$\n",
    "        * $x_3^{s+1} \\sim p(x_3 | x_1^s, x_2^s)$\n",
    "        \n",
    "    * The expression $p(x_i | x_{-i})$ is called the **full conditional** for variable $i$\n",
    "    \n",
    "    * It is usually necessary to discard some of the initial samples until the Markov Chain has **burned in**.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](gibbs.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis-Hastings algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Models where we can compute the full conditional probability are limited\n",
    "\n",
    "* Even when this exists, Gibbs sampling can be quite slow\n",
    "\n",
    "* More general algorithm is the **Metropolis Hastings (MH)** algorithm.\n",
    "\n",
    "* **Basic idea**:\n",
    "\n",
    "    * at each step, we propose to move from the current state $x$ to a new state $x'$ with probability $q(x'|x)$, where $q$ is called the **proposal distribution** or **kernel**.\n",
    "    \n",
    "    * the user is free to use any kind of proposal they want, subject to some conditions\n",
    "    \n",
    "    * Commonly used proposal: symmetric Gaussian distribution centered on the current state: $q(x'|x) = N(x'|x, \\Sigma)$; which is called a **random walk MH**\n",
    "    \n",
    "    * if we use a proposal of the form $q(x'|x) = q(x')$, the new state is independent of the old state, and the method is known as **independence sampler**.\n",
    "    \n",
    "    * having proposed $x'$, we then decide to accept or reject it according to some formula, which ensures that the fraction of time spent in each state is proportional to $p^*(x)$\n",
    "    \n",
    "    * if the proposal is accepted, the new state is $x'$, otherwise the state remains the same.\n",
    "    \n",
    "    * if the proposal is symmetric, i.e. $q(x'|x) = q(x | x')$ the acceptance probability is given by the following formula:\n",
    "    \n",
    "        * $r = \\min (1, \\frac{p^*(x')}{p^*(x)})$\n",
    "        \n",
    "        * i.e. we always move to $x'$ if it is more probable than $x$, otherwise we move there with some probability.\n",
    "        \n",
    "    * **It can be shown that this procedure ensures that the fraction of time we spend in each state $x$ is proportional to $p^*(x)$.**\n",
    "    \n",
    "    * Note that if the proposal is asymmetric, so $q(x'|x) \\ne q(x|x')$, we need the **Hasting correction**:\n",
    "    \n",
    "        * $r = \\min(1, \\alpha)$\n",
    "        \n",
    "        * $\\alpha = \\frac{p^*(x') q(x|x')}{p^*(x) q(x'|x)} = \\frac{p^*(x')/q(x'|x)}{p^*(x) / q(x|x')}$\n",
    "        \n",
    "* An important reason why MH is useful is that, when evaluating for $\\alpha$, we only need to know the target density up to a normalization constant (which gets canceled in the formula above)\n",
    "      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](MH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](3dRosenbrock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMC https://docs.pymc.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](MCMC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](MCMC2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](MCMC3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](MCMC4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine invariant MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method popularized by the **emcee** code http://dfm.io/emcee/current/\n",
    "\n",
    "Paper from Goodman & Weare in here: https://msp.org/camcos/2010/5-1/camcos-v5-n1-p04-p.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic idea**:\n",
    "    \n",
    "* The method has many **parallel walkers**, each sampling from the distribution\n",
    "\n",
    "* the walkers are only allowed to do linear combinations of their solutions, which simplifies significantly the theoretical derivation (**stretch move**)\n",
    "\n",
    "* usually much faster than MH\n",
    "\n",
    "* Proposal of the form $x_k(t) \\rightarrow Y = x_j + z(x_k(t) - x_j)$\n",
    "\n",
    "* If Z satisfies the symmetry condition: $g(\\frac{1}{z}) = z g(z)$, then the move is symmetric in the sense that: $p(x_k(t) \\rightarrow Y) = p(y \\rightarrow x_k(t))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](stretch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the particular distribution used is the  following:\n",
    "\n",
    "## $g(z)=  \\begin{cases} \\frac{1}{\\sqrt{z}} & \\text{if} z \\in [\\frac{1}{a}, a] \\\\ 0, & \\text{otherwise} \\end{cases}$\n",
    "\n",
    "where the parameter $a > 1$ can be adjusted to improve performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](stretch2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using SN data from the HiTS survey (Förster et al. 2016) fitted to hydrodynamical models from Moriya et al. 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](evol15X.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](corner15X.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](models15X.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
