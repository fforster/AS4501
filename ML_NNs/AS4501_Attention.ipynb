{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2270db60",
   "metadata": {},
   "source": [
    "$\\Huge AS4501$\n",
    "\n",
    "Transformers and Attention\n",
    "\n",
    "Francisco Förster\n",
    "\n",
    "Bibliography:\n",
    "\n",
    "* [Attention is all you need, Vaswani et al. 2017](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "* https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html (many figures from this great website)\n",
    "* https://towardsdatascience.com/attention-and-transformer-models-fe667f958378"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8fec16",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21074456",
   "metadata": {},
   "source": [
    "Recurrent neural networks have two big problems:\n",
    "\n",
    "1. They tend to give too much weight to recent elements in a sequence, but sometimes the most important connections in a sentence are separated by a large number of elements.\n",
    "\n",
    "2. They are intrinsically serial in nature. We need to process a sequence in order to compute the output of a RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159403d3",
   "metadata": {},
   "source": [
    "This is how a RNN processes a sentence, paying more attention to the last word at each step and requiring a serial processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c14a9",
   "metadata": {},
   "source": [
    "![](images/sentence-classification-rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1163acd7",
   "metadata": {},
   "source": [
    "But in many cases the last word is not the most important, and we would like to be able to process each word and its association with other words in parallel:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e6bcf",
   "metadata": {},
   "source": [
    "![](images/sentence-example-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca6345",
   "metadata": {},
   "source": [
    "This also happens in the problem of translation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc00b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-27T03:26:40.740198Z",
     "start_time": "2023-05-27T03:26:40.629069Z"
    }
   },
   "source": [
    "![](images/sentence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17541e4",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d353c",
   "metadata": {},
   "source": [
    "Let's remember the softmax function applied to a vector x:\n",
    "\n",
    "$\\Large {\\rm softmax(x_i)} = \\frac{\\exp{x_i}}{\\sum\\limits_j \\exp{x_j}}$ \n",
    "\n",
    "This function returns ~1 at the largest value of the vector and ~0 elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea07efb8",
   "metadata": {},
   "source": [
    "![](images/softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da897d4",
   "metadata": {},
   "source": [
    "# Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c003461",
   "metadata": {},
   "source": [
    "The attention mechanism is an approach in deep learning that allows models to focus on different parts of the input when producing the output. Instead of focusing in some hidden state like in RNNs, in attention each output explicitly depends on all previous input states, weighted by attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cca037",
   "metadata": {},
   "source": [
    "For example in this sentence with the following attention scores:\n",
    "\n",
    " I love travelling\n",
    "   \n",
    "   [0.1,  0.2,  0.7] ---> J'adore\n",
    "  \n",
    "  [0.5,  0.5,  0.0] ---> voyager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3357e895",
   "metadata": {},
   "source": [
    "'J'adore' pays more attention or has more affinity to 'travelling' as the next word when translating.\n",
    "\n",
    "'voyager' pays attention to 'I' and 'love' equally when translating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e959d13a",
   "metadata": {},
   "source": [
    "# Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a38367",
   "metadata": {},
   "source": [
    "Self Attention, also known as intra Attention, is an attention mechanism that relates different positions of one sequence in order to compute a representation of the same sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec836ef",
   "metadata": {},
   "source": [
    "![](images/intraattention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9744b86",
   "metadata": {},
   "source": [
    "In a self-attention layer, an input matrix $X$ ($n$ tokens of dimension $d$) are turned it into an output matrix $Z$ ($n$ components of dimension $d_v$) via three representational matrices of the input:\n",
    "\n",
    "* queries Q\n",
    "* keys K\n",
    "* values V\n",
    "\n",
    "$\\Large {\\rm Attention}(Q, K, V) = {\\rm softmax}( Q \\cdot K^T / \\sqrt{d_k}) * V$\n",
    "\n",
    "where $Q$, $K$ and $V$ are matrices representing linear transformations from the input vector $x$ via learnable parameters $W^Q$, $W^K$ and $W^V$:\n",
    "\n",
    "* $Q = X W^Q$\n",
    "* $K = X W^K$\n",
    "* $V = X W^V$\n",
    "\n",
    "Note that \n",
    "* $x \\in \\mathbb{R}^{n \\times d}$\n",
    "* $Q \\in \\mathbb{R}^{n \\times d_k}$\n",
    "* $K \\in \\mathbb{R}^{n \\times d_k}$\n",
    "* $V \\in \\mathbb{R}^{n \\times d_v}$\n",
    "* $W^Q \\in \\mathbb{R}^{d \\times d_k}$\n",
    "* $W^K \\in \\mathbb{R}^{d \\times d_k}$\n",
    "* $W^V \\in \\mathbb{R}^{d_v \\times d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576fa1d",
   "metadata": {},
   "source": [
    "![](images/attention_detail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb2033",
   "metadata": {},
   "source": [
    "![](images/selfattention_summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ac3b1",
   "metadata": {},
   "source": [
    "# Cross-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11692cd",
   "metadata": {},
   "source": [
    "One can generalize the previous computation for combining two input matrices $X_1$ and $X_2$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5325723d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T02:13:31.845022Z",
     "start_time": "2023-06-02T02:13:31.724947Z"
    }
   },
   "source": [
    "![](images/cross-attention-summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0293866",
   "metadata": {},
   "source": [
    "And this is an example of a cross attention matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a499b",
   "metadata": {},
   "source": [
    "![](images/bahdanau-fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e27a8d",
   "metadata": {},
   "source": [
    "and a visualization of one row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482beb54",
   "metadata": {},
   "source": [
    "![](images/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca7086",
   "metadata": {},
   "source": [
    "# Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f37112",
   "metadata": {},
   "source": [
    "In multi-head attention we concatenate the output from several heads $i$ with learnable parameters $W_i^Q$, $W_i^K$ and $W_i^V$, and then linearly transform this vector with learnable parameters $W^O$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac31057",
   "metadata": {},
   "source": [
    "$\\Large {\\rm Multihead} = {\\rm concat}({\\rm head}_1, ... {\\rm head}_h) W^O$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d054c188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-02T02:23:57.613635Z",
     "start_time": "2023-06-02T02:23:57.491341Z"
    }
   },
   "source": [
    "![](images/multi-head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78933c46",
   "metadata": {},
   "source": [
    "# Positional encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0a446",
   "metadata": {},
   "source": [
    "One problem with the previous strategy is that the order of the input is never used to compute the attention scores. In order to fix this problem, information about the relative positions of the inputs must be added. In the original paper by Vaswani they use sine and cosine functions of different frequencies:\n",
    "\n",
    "* $PE(pos, 2i) = sin(pos / 10000^{2i/d})$\n",
    "* $PE(pos, 2i) = cos(pos / 10000^{2i/d})$\n",
    "\n",
    "![](images/PE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab936cce",
   "metadata": {},
   "source": [
    "In other works, a set of functions are learned as the positional encoder. For example, in [Pimentel+2023](https://arxiv.org/pdf/2201.08482.pdf) they use the following function (timeFiLM):\n",
    "\n",
    "![](images/timefilm.png)\n",
    "![](images/timefilm2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d009ed",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b030c",
   "metadata": {},
   "source": [
    "The full transformer arquitecture proposed by Vaswani et al. 2017 is the following:\n",
    "\n",
    "![](images/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba289a",
   "metadata": {},
   "source": [
    "The model is composed of an encoder and a decoder. \n",
    "\n",
    "The encoder is composed of 6 identical layers, each one with two sublayers: a multi-head self-attention mechanism and a position wise fully connected feed-forward network. The output of each sublayer uses a residual connection (we add the input to the output of the sublayer), which helps with convergence, and is normalized using layer normalization.\n",
    "\n",
    "The decoder is also composed of 6 identical layers. In addition to the two sublayers used in the encoder, a sublayer is added in between that uses multihead cross attention with the output of the encoder. The multihead self-attention is also modified to mask positions that have not been visited by the decoder (predictions for position i can depend only on the known outputs of positions less than i).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be9015",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "574c7ace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:39:56.064850Z",
     "start_time": "2024-11-12T13:39:55.760200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>\"Western Union\" is something of a forgotten cl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>This movie is an incredible piece of work. It ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>My wife and I watched this movie because we pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>When I first watched Flatliners, I was amazed....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>Why would this film be so good, but only gross...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      I grew up (b. 1965) watching and loving the Th...      0\n",
       "1      When I put this movie in my DVD player, and sa...      0\n",
       "2      Why do people who do not know what a particula...      0\n",
       "3      Even though I have great interest in Biblical ...      0\n",
       "4      Im a die hard Dads Army fan and nothing will e...      1\n",
       "...                                                  ...    ...\n",
       "39995  \"Western Union\" is something of a forgotten cl...      1\n",
       "39996  This movie is an incredible piece of work. It ...      1\n",
       "39997  My wife and I watched this movie because we pl...      0\n",
       "39998  When I first watched Flatliners, I was amazed....      1\n",
       "39999  Why would this film be so good, but only gross...      1\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Load IMDb dataset\n",
    "df = pd.read_csv(\"IMDB.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57724e3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:39:56.231934Z",
     "start_time": "2024-11-12T13:39:56.227043Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3915da72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:39:57.699977Z",
     "start_time": "2024-11-12T13:39:56.352061Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic text preprocessing and tokenization\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "# Build vocabulary from the training data\n",
    "def build_vocab(texts, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(tokenize(text))\n",
    "    return {word: idx+1 for idx, (word, count) in enumerate(counter.items()) if count >= min_freq}\n",
    "\n",
    "vocab = build_vocab(train_df['text'].values)\n",
    "vocab_size = len(vocab) + 1  # +1 for padding token\n",
    "\n",
    "def text_to_indices(text, vocab, max_len=100):\n",
    "    tokens = tokenize(text)\n",
    "    indices = [vocab.get(token, 0) for token in tokens][:max_len]\n",
    "    indices = [min(idx, len(vocab)) for idx in indices]  # Ensure all indices are within vocabulary size\n",
    "    indices += [0] * (max_len - len(indices))  # Pad sequences shorter than max_len\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "faccb892",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:40:00.134598Z",
     "start_time": "2024-11-12T13:39:57.828153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data with indices for train and test sets\n",
    "train_df['indices'] = train_df['text'].apply(lambda x: text_to_indices(x, vocab))\n",
    "test_df['indices'] = test_df['text'].apply(lambda x: text_to_indices(x, vocab))\n",
    "train_df['label'] = train_df['label'].apply(torch.tensor)\n",
    "test_df['label'] = test_df['label'].apply(torch.tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da49907c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:40:00.324016Z",
     "start_time": "2024-11-12T13:40:00.308819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14307</th>\n",
       "      <td>I watched it last night and again this morning...</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17812</th>\n",
       "      <td>although i liked this Western,i do have to say...</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>[63, 1, 13, 8, 88, 46, 74, 24, 89, 47, 90, 45,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11020</th>\n",
       "      <td>I sat down to watch a documentary about Puerto...</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>[1, 147, 148, 24, 149, 66, 150, 17, 151, 152, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15158</th>\n",
       "      <td>This was probably intended as an \"arty\" crime ...</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>[8, 21, 287, 288, 72, 204, 289, 290, 291, 84, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>The summary provided by my cable TV guide made...</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>[20, 318, 319, 117, 31, 320, 321, 322, 323, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>This movie is one of the worst movie i have ev...</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>[8, 18, 15, 90, 45, 20, 331, 18, 1, 74, 334, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>This movie is inspiring to anyone who is or ha...</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>[8, 18, 15, 2918, 24, 1503, 40, 15, 167, 490, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38158</th>\n",
       "      <td>\"East Side Story\" is a documentary of musical ...</td>\n",
       "      <td>tensor(1)</td>\n",
       "      <td>[9339, 541, 87, 15, 66, 150, 45, 1083, 1072, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>And a self-admitted one to boot. At one point ...</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>[6, 66, 0, 90, 24, 4822, 313, 90, 2721, 20, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>This movie had horrible lighting and terrible ...</td>\n",
       "      <td>tensor(0)</td>\n",
       "      <td>[8, 18, 433, 3168, 3421, 6, 1550, 1046, 2943, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text      label  \\\n",
       "14307  I watched it last night and again this morning...  tensor(1)   \n",
       "17812  although i liked this Western,i do have to say...  tensor(1)   \n",
       "11020  I sat down to watch a documentary about Puerto...  tensor(0)   \n",
       "15158  This was probably intended as an \"arty\" crime ...  tensor(0)   \n",
       "24990  The summary provided by my cable TV guide made...  tensor(0)   \n",
       "...                                                  ...        ...   \n",
       "6265   This movie is one of the worst movie i have ev...  tensor(0)   \n",
       "11284  This movie is inspiring to anyone who is or ha...  tensor(1)   \n",
       "38158  \"East Side Story\" is a documentary of musical ...  tensor(1)   \n",
       "860    And a self-admitted one to boot. At one point ...  tensor(0)   \n",
       "15795  This movie had horrible lighting and terrible ...  tensor(0)   \n",
       "\n",
       "                                                 indices  \n",
       "14307  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 13,...  \n",
       "17812  [63, 1, 13, 8, 88, 46, 74, 24, 89, 47, 90, 45,...  \n",
       "11020  [1, 147, 148, 24, 149, 66, 150, 17, 151, 152, ...  \n",
       "15158  [8, 21, 287, 288, 72, 204, 289, 290, 291, 84, ...  \n",
       "24990  [20, 318, 319, 117, 31, 320, 321, 322, 323, 3,...  \n",
       "...                                                  ...  \n",
       "6265   [8, 18, 15, 90, 45, 20, 331, 18, 1, 74, 334, 1...  \n",
       "11284  [8, 18, 15, 2918, 24, 1503, 40, 15, 167, 490, ...  \n",
       "38158  [9339, 541, 87, 15, 66, 150, 45, 1083, 1072, 5...  \n",
       "860    [6, 66, 0, 90, 24, 4822, 313, 90, 2721, 20, 12...  \n",
       "15795  [8, 18, 433, 3168, 3421, 6, 1550, 1046, 2943, ...  \n",
       "\n",
       "[32000 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "275e9be5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:40:00.493439Z",
     "start_time": "2024-11-12T13:40:00.485689Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom Dataset class for DataLoader\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.reviews = df['indices'].values\n",
    "        self.labels = df['label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.reviews[idx]), self.labels[idx]\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = IMDBDataset(train_df)\n",
    "test_dataset = IMDBDataset(test_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afc0b67e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:40:00.669261Z",
     "start_time": "2024-11-12T13:40:00.644825Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Define the TransformerEncoderLayer class\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.multi_head_attn = nn.MultiheadAttention(d_model, num_heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply multi-head attention and add residual connection\n",
    "        attn_output, _ = self.multi_head_attn(x, x, x)\n",
    "        x = self.layer_norm1(x + attn_output)\n",
    "\n",
    "        # Apply feed-forward network and add residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.layer_norm2(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Define the SimpleTransformerClassifier model using the TransformerEncoderLayer\n",
    "class SimpleTransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, dim_feedforward, num_layers, num_classes):\n",
    "        super(SimpleTransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, dim_feedforward) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(1, 0, 2)  # Embed and transpose for transformer layer\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        # Pooling and final classification layer\n",
    "        x = x.permute(1, 2, 0)  # Reshape to (batch_size, d_model, seq_length) for pooling\n",
    "        x = self.pool(x).squeeze(-1)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Model parameters\n",
    "d_model = 64  # Reduced dimension for faster training\n",
    "num_heads = 2\n",
    "dim_feedforward = 128\n",
    "num_layers = 1\n",
    "num_classes = 2\n",
    "\n",
    "# Instantiate the model\n",
    "vocab_size = len(vocab) + 1  # from previous vocab creation\n",
    "model = SimpleTransformerClassifier(vocab_size, d_model, num_heads, dim_feedforward, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3539c26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:42:18.784670Z",
     "start_time": "2024-11-12T13:40:00.824958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.5220, Accuracy: 0.7304\n",
      "Epoch [2/5], Loss: 0.3699, Accuracy: 0.8357\n",
      "Epoch [3/5], Loss: 0.2884, Accuracy: 0.8807\n",
      "Epoch [4/5], Loss: 0.2190, Accuracy: 0.9163\n",
      "Epoch [5/5], Loss: 0.1574, Accuracy: 0.9430\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for texts, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf646b60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:42:20.495974Z",
     "start_time": "2024-11-12T13:42:19.096220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8204\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66547a",
   "metadata": {},
   "source": [
    "## Fine tuning [BERT](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73fbd36",
   "metadata": {},
   "source": [
    "![](images/bert.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ca7c21b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:42:21.474948Z",
     "start_time": "2024-11-12T13:42:20.762939Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ba9ba2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:42:23.914596Z",
     "start_time": "2024-11-12T13:42:21.694234Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fforster/anaconda3/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/fforster/anaconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05349230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:42:24.124784Z",
     "start_time": "2024-11-12T13:42:24.122398Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example data: a list of sentences and their corresponding labels\n",
    "sentences = [\n",
    "    \"I love programming.\", \"The weather is great today!\", \"I'm feeling sad.\",\n",
    "    \"It's a beautiful day.\", \"I hate traffic.\", \"Coding is fun.\", \"I enjoy sunny days.\",\n",
    "    \"It's raining cats and dogs.\", \"I am very excited.\", \"I feel disappointed.\"\n",
    "]\n",
    "\n",
    "labels = [1, 1, 0, 1, 0, 1, 1, 0, 1, 0]  # 1 = positive, 0 = negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "611ec843",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:42:24.411439Z",
     "start_time": "2024-11-12T13:42:24.404556Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the sentences\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edd75eec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:42:24.710876Z",
     "start_time": "2024-11-12T13:42:24.707464Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a TensorDataset and DataLoader\n",
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], torch.tensor(labels))\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce4beee7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:42:24.952972Z",
     "start_time": "2024-11-12T13:42:24.950178Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "380a73c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:42:31.236652Z",
     "start_time": "2024-11-12T13:42:25.159801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.7311, Validation Loss: 0.6165\n",
      "Epoch: 2, Train Loss: 0.6910, Validation Loss: 0.6253\n",
      "Epoch: 3, Train Loss: 0.6891, Validation Loss: 0.6228\n",
      "Epoch: 4, Train Loss: 0.6692, Validation Loss: 0.6177\n",
      "Epoch: 5, Train Loss: 0.6328, Validation Loss: 0.6045\n",
      "Epoch: 6, Train Loss: 0.5852, Validation Loss: 0.5947\n",
      "Epoch: 7, Train Loss: 0.5514, Validation Loss: 0.5858\n",
      "Epoch: 8, Train Loss: 0.5903, Validation Loss: 0.5769\n",
      "Epoch: 9, Train Loss: 0.5682, Validation Loss: 0.5694\n",
      "Epoch: 10, Train Loss: 0.5748, Validation Loss: 0.5605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop with validation\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):  # Training for more epochs\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba9a518e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:44:40.788055Z",
     "start_time": "2024-11-12T13:44:40.745930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer([\"I love sunny days.\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    print(f\"Predicted label: {predictions.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a41e85",
   "metadata": {},
   "source": [
    "## Vision transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95479e7",
   "metadata": {},
   "source": [
    "This is based on the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n",
    "](https://arxiv.org/abs/2010.11929)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a61305",
   "metadata": {},
   "source": [
    "![](images/vit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c63822",
   "metadata": {},
   "source": [
    "See https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20fd5be7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:44:50.577578Z",
     "start_time": "2024-11-12T13:44:49.163133Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5028784",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:44:53.289954Z",
     "start_time": "2024-11-12T13:44:53.287481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the transformations for the training and validation sets\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0dc663a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:44:57.143079Z",
     "start_time": "2024-11-12T13:44:55.788368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoader objects for the training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a50e8780",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:45:00.636432Z",
     "start_time": "2024-11-12T13:44:59.827582Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fforster/anaconda3/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/fforster/anaconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained ViT model for image classification\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=10, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "073b27a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:45:03.154811Z",
     "start_time": "2024-11-12T13:45:03.143951Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76d9144a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T13:45:06.032124Z",
     "start_time": "2024-11-12T13:45:06.026296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bdc74a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-12T13:45:09.175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    ibatch = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        print(ibatch, end='\\r')\n",
    "        ibatch += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs).logits\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Validation Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f9759",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-24T14:45:50.514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine-tuned-vit')\n",
    "\n",
    "# Helper function to display images along with their predicted labels\n",
    "def imshow(img, title):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Class labels for CIFAR-10\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Get some random validation images\n",
    "dataiter = iter(val_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Move the images to the GPU if available\n",
    "images = images.to(device)\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(images).logits\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Convert images and predictions back to CPU for visualization\n",
    "images = images.cpu()\n",
    "predicted = predicted.cpu()\n",
    "\n",
    "# Show the images along with their predicted labels\n",
    "for i in range(4):  # Display 4 examples\n",
    "    imshow(images[i], f'Predicted: {classes[predicted[i]]} | True: {classes[labels[i]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b688f64c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
